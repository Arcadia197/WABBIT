;------------------------------------------------------------
;------------ WABBIT PARAMETER FILE TEMPLATE ----------------
;------------------------------------------------------------
; if you add new parameters, add them here.
; note values have to be declared "value=0;", with equal sign (=) and semicolon (;)


[Domain]
; 2D or 3D problem?
dim=2;
; box size of computational domain. [Lx Ly Lz]
domain_size=20 20;
; synchronization (on/off)on [x y z] domain boundaries
; (off (NON-PERIODIC): 0/false/yes | on (PERIODIC): 1/true/no)
periodic_BC=1 1;


[Blocks]
; size of each block, must be odd (17, 19, 33, etc), if given one value this is used
; for all directions, or specify value for each direction
; Note: in 04/2020, I changed the grid definition slightly to remove the redundant point
;   from the grid. Now each point is contained exactly one time. For postprocessing, it is often
;   desirable to include the redundant point, which is now simply the first ghost node. The code
;   thus stores blocks of size (Bs+1), i.e., in output h5 files, the array size is larger.
;   Old files generated before 04/2020 can be read just the same, but the value for Bs must be set
;   to the old value - 1 (before Bs=17, now Bs=16). Error is thrown otherwise.
number_block_nodes=26;
; ghost nodes for each block. you need to know this value depending on the discretization and the multiresolution
; interpolation.
number_ghost_nodes=;
number_ghost_nodes_rhs=;
; number of equations / components of state vector. Note you have to properly
; adjust this value for the physics module that you use.
; ACM: 3 (2D), 4 (3D) +  nscalars
; Convection: 1 (2D /3D)
number_equations=3;
; threshold value for thresholding wavelet coefficients
eps=1.0e-3;
; treelevel bounds: determine the highest (max_treelevel) and lowest (min_treelevel) refinement
; level of blocks. With each refinement level the grid gets refined by a factor of two. The maximum
; resolution at max_treelevel is dx = 2^-max_treelevel * L / Bs (L:domain size, Bs: blocksize)
max_treelevel=6;
; sometimes you want to restrict the minimum refinement as well, for example if we run equidistant
; simulations (which is quite rare!): in that case, max_treelevel = min_treelevel
min_treelevel=;
; max number of trees in the forest. A tree corresponds to a grid; this notation
; only is important for postprocessing, i.e. using the adaptive-POD module. For running
; simulations, you can leave the value empty - it is set automatically.
max_forest_size=;
; Use adaptivity or not? If adapt_mesh=1 then the grid is refined before a time step,
; the time evolution is done, and the grid is coarsened after the time step. If adapt_mesh=0
; the grid does not change after the initial condition, and refinement/coarsening are disabled.
adapt_mesh=1;
; adaptive initial conditon? i.e. create grid to respect error bounds
; default is same value as adapt_mesh
adapt_inicond=1;
; in some situations, it is necessary to create the intial grid, and then refine it for a couple of times.
; for example if one does non-adaptive non-equidistant spatial convergence tests. default is 0.
inicond_refinements=0;
; block distribution for balancing (also used for start distribution)
; [sfc_z | sfc_hilbert]
; sfc_z  -> space filling curve -> z-curve
; sfc_hilbert -> hilbert space filling curve
block_dist=sfc_hilbert;
; refinement indicator to be used in mesh refinement [everywhere, significant]
; everywhere: refine every single block, a long proven tactic
; significant: do not refine blocks which are not deemed significant, this can save many blocks but is still experimental
refinement_indicator=everywhere;
; coarsening indicator to be used in mesh adaptation (=coarsening) [threshold-state-vector, random, primary-variables]
; threshold-state-vector: evaluates wavelet criterion on components of state vector. specify below which ones.
; primary-variables: only available for NStokes: converts statevector to (rho,u,v,w,p) before thresholding
; randomly coarse some blocks. used for testing. note we tag for coarsening only once in the first iteration
coarsening_indicator=threshold-state-vector;
; use normalization for eps or not? Thresholding means we control the absolute error in some norm (often Linfty norm, see below)
; but very often, you rather want to control the relative error. So, some norm of the field is computed, such that
; || u-u_eps || / ||u|| < eps  (using the norm specified below)
; default=0, even though this is stupid: if you have large pressure (say u=1 and pressure=10000), then only the pressure will determine
; the grid
eps_normalized=1;
; norm where the thresholding is applied - Linfty (default), L2 or H1
eps_norm=Linfty;
; which components to use for coarsening_indicator? default is all components.
; active only if coarsening_indicator=threshold-state-vector. select the components, set as
; many as number_equations
threshold_state_vector_component=1 1 1;
; it can be useful to also use the mask function (if penalization is used) for grid adaptation.
; i.e. the grid is always at the finest level on mask interfaces. Careful though: the Penalization
; is implemented on physics-module level, i.e. it is not available for all modules.
threshold_mask=1;
; if this flag is set (1), then blocks on max level have to coarsen, even if their
; details are significant. This is equivalent to ensuring dealiasing. Hence, if set to 1,
; wabbit will evaluate the right hand side of your equation on max_treelevel, but in the mesh
; coarsening it will, regardless of the solution, downsample the result to max_treelevel-1. Your
; expected precision is thus max_treelevel-1, but the computational cost (derivatives and timestep)
; is on max_treelevel.
force_maxlevel_dealiasing=1;
; if desired, we perform more than one time step
; before adapting the grid again. this can further reduce the overhead of adaptivity
; Note: the non-linear terms can create finer scales than resolved on the grid. they
; are usually filtered by the coarsening/refinement round trip. So if you do more than one time step
; on the grid, consider using a filter. default is "1", which is the classical scheme
N_dt_per_grid=1;


[Wavelet]
wavelet=CDF44;


[Time]
; final time to reach in simulation
time_max=0.1;
; maximum walltime allowed for simulations (in hours). The run will be stopped if this duration
; is exceeded. This is useful on real clusters, where the walltime of a job is limited, and the
; system kills the job regardless of whether we're done or not. If WABBIT itself ends execution,
; a backup is written and you can resume the simulation right where it stopped. Note you can also
; stop a run using the file "runtime_control" (set runtime_control=save_stop;)
walltime_max=4.9;
; While we can save every write_time time units of the simulation, it may be desirable to save also
; every couple of hours of runtime (for very expensive runs) so that we can be sure to be able to
; resume the simulation with at most the loss of these hours. default unused, set value in HOURS.
walltime_write=;
; number of time steps performed. if not set, default value is very large
nt=;
; CFL criterium (velocity). Note the time step dt is dictated by the physics modules: some eqns (like
; the heat eqn, which is not implemented) may not even have a CFL restriction.
CFL=1.5;
; CFL critierum for penalization (dt<=CFL_eta*C_eta), if VPM is used. For RungeKuttaGeneric schemes, the constant
; has to be < 1.0 (otherwise the code is unstable). For krylov schemes, it can be greater
; 1, but be careful about the error. This parameter is used by ACM physics module only.
CFL_eta=0.99;
; time step restriction of viscous terms ( dt < CFL_NU * dx**2 / nu )
; runge kutta 4 has constraints: 2D=>0.14 3D=>0.094 (exact expression: 2.79/(dim*pi**2)), these are
; the default values
CFL_nu=;
; wabbit can save the heavy data (flow fiels) to HDF5. What is saved depends on the physics modules
; and the section [Saving]. Here you control WHEN you want to save the output: either after a fixed
; number of time steps [fixed_freq], or after a physical time interval [fixed_time]
write_method=fixed_time;
; if write_method=fixed_freq:
; write frequency for output, choose very large number for disabling output on disk
write_freq=2;
; if write_method=fixed_time:
; write time for output
write_time=0.05;
; fixed time step. if the value is greater 0.0, then the time step is fixed no matter what.
; the setting from the physics modules, which usually decide about dt, are ignored and over-
; written. The default is 0.0, so not used. NOTE: WABBIT still will adjust dt to precisely match
; the time for saving and statistics and the final time, if any of those is not a multiple of dt_fixed.
; In that case, some time steps may be smaller in order to reach those times.
dt_fixed=0.0;
; largest time step, if you want to set one. dt is always smaller than that, if the
; value is greater 0. default is 0.0, so not used. WABBIT overwrites the physics module dt
; by that value, if the timestep is larger than dt_max and dt_max > 0.
dt_max=0.0;
; time-step method. can be either "RungeKuttaGeneric" or "Krylov". In the former case,
; any explicit Runge-Kutta scheme can be set by using the Butcher-Tableau. (RK4 is default) In the latter,
; the number of Krylov subspaces M_krylov can be set.
; [ RungeKuttaGeneric, Krylov, RungeKuttaChebychev ]
time_step_method=RungeKuttaGeneric;
; The usual RungeKuttaChebychev method contains a number of hard coded schemes for different
; number of stages "s" and the fixed damping coefficient eps=10.0 (see Verwer and Sommeijer)
; if you want to use another RKC scheme, you can do so by settting it up in this INI-file.
; Provide coefficients with length "s". they are usually precomputed in python.
RKC_custom_scheme=no;
; (complicated) coefficients following Verwer & Sommeijer follow.
; longest scheme is 60 stages.
RKC_mu=;
RKC_mu_tilde=
RKC_nu=
RKC_gamma_tilde=
RKC_c=
; number of stages "s" for the RungeKuttaChebychev method. Memory is always 6 registers
; independent of stages. Default code allows up to 40 stages for eps=10.0 (hard-coded)
; coefficients. other schemes can be defined manually above.
s=10;
; if time_step_method is krylov, then you can specify the dimension of the krylov subspace
; below. If dynamic subspace dimensions are used, we interpret this number as the maximum
; number of spaces admissible (the method requires a lot of memory in general)
M_krylov=12;
; fixed or dynamic krylov subspace dimension:
; [ fixed, dynamic ]
krylov_subspace_dimension=fixed;
; if dynamic subspace dimensionality is used, provide the residuum threshold here. Note this is
; in general not an exact measure for the error, but rather a good indicator.
krylov_err_threshold=1.0e-3;
; butcher_tableau
; use your butcher_tableau for the Runge Kutta time step function
; e.g. RK4:
; butcher_tableau=(/ 0.0 0.0 0.0 0.0 0.0
; 0.5 0.5 0.0 0.0 0.0
; 0.5 0.0 0.5 0.0 0.0
; 1.0 0.0 0.0 1.0 0.0
; 0.0 0.16666666666666666 0.33333333333333331 0.33333333333333331  0.16666666666666666 /)


[Physics]
; what physics module is used?
; [ACM-new, ConvDiff-new, navier_stokes]
physics_type=ACM-new;
; decide if you want to start from a given configuration (i.e. Statevector)
; 1:true, 0:false and we start from the initial conditions dictated by the physics
; modue.
read_from_files=0;
; if read_from_files is true, WABBIT will try to start from the given files
input_files=ux_000002700000.h5 uy_000002700000.h5 p_000002700000.h5;


[Saving]
; WABBIT is in charge of saving, but what is saved is controled by the physics modules.
; here, you need to tell WABBIT how many fields are saved and how they will be labeled.
; The physics modules are then in charge of providing the respective data to WABBIT. I.e.
; if the field is called "mask", WABBIT will ask the physics module to return the array
; "mask" and then save that to disk.
; how many fields are you going to save?
N_fields_saved=4;
; how are the fields labeled?
field_names=ux uy p mask;


[Statistics]
; save every nsave time steps (leave empty to disable)
nsave_stats=10;
; and every tsave physical time units (leave empty to disable)
tsave_stats=0.20;


[ACM-new]
; speed of sound, used also for the CFL condition. The characteristic velocity is
; u_eigen = umax + sqrt(umax**2 + c0**2)
c_0=12.5;
; viscosity. We also respect the time step constraint imposed by explicit diffusion
nu=0.0;
; damping term for pressure, see [Ohwada, Asinari, JCP2010]
gamma_p=0;
; mean flow, is imposed as initial condition (if inicond=meanflow) and in the sponge
; layer as far-field BC (if this is used!)
;u_mean_set=0.0 +1.0 0.0;
u_mean_set=0.0 -1.0 0.0;
; initial condition
inicond=meanflow;
; if inicond= pressure-blob
beta=0.05;
; we can use passive scalars with the ACM. their parameters are specified in "ConvectionDiffusion" below
; but they need to be turned on here. Default is off.
use_passive_scalar=0;
; sometimes, but very rarely, we want to compute just the passive scalar and not
; the flow field, because the convection-diffusion physics module does not contain the
; penalized passive scalar eqns.
compute_flow=1;

nu_p=0.0;

use_HIT_linear_forcing=0;


[Sponge]
; sponge term, used in ACM module to mimick outflow conditions for pressure waves
; NOTE (ACM): when the sponge is used, be sure to allow for mask_time_dependent_part=1. Even
; though the sponge function is not explicitly time-dependent, it is treated as such because
; the sponge does not have to be at the maximum refinement level.
use_sponge=1;
; shape of the sponge. the tradional form (default) is "rect", but we have a smoothed
; version with round corners which is called "p-norm". It has the parameter p_sponge,
; which controls the roundedness of corners. The function is inspired by the p-norm
; ((x-x0)**p + (y-y0)**p + (z-z0)**p)**1/p [https://de.wikipedia.org/wiki/P-Norm]
; a good compromise is p=20.0
sponge_type=p-norm;
; p-sponge is used only in conjunction with sponge_type=p-norm;
p_sponge=8.0;
; thickness of sponge layer (in physial units)
L_sponge=2.0;
; sponge damping constant
C_sponge=8.0e-3;


[ConvectionDiffusion]
N_scalars=0;


[Discretization]
; order of derivatives [ FD_2nd_central | FD_4th_central_optimized ]
order_discretization=FD_4th_central;
filter_type=no_filter;wavelet_filter;
filter_freq=1;

[VPM]
; Volume penalization method to take obstacles into account without adapting the
; grid. Penalization is an equation-dependent addon. It can not be used with any
; RHS, if the RHS does not support it.
; flag for penalization (0/1)
penalization=1;
; WABBIT needs to know about the mask function (if penalization is used): does it contain
; a time-dependent-part (e.g. moving obstacles, time-dependent forcing)? does it contain
; a time-independent part (fixed walls, homogeneous forcing)? or both? WABBIT needs to know
; that since we try to create the time-independent mask function only once, but the time-dependent
; part of course in every time step.
; NOTE: the flag threshold_mask has a critical impact here: if it is false, it is not guaranteed that the
; fluid-solid interface is on Jmax or Jmax-1, hence the technique of using a pre-generated static mask function
; that can be added to the time-dependent part cannot be used. If the mask function is expensive to compute,
; ensure that threshold_mask is set.
mask_time_dependent_part=1;
mask_time_independent_part=1;
;
dont_use_pruned_tree_mask=1;
; smooth mask for penalization term [0,1]. If the obstacle moves, this flag should
; be set, and in static cases as well. hard masks with only 0 and 1 are deprecated but
; included for completeness. Note some mask functions may ignore this flag (the insects for example)
smooth_mask=1;
; penalization factor. Can be seen as porosity, so smaller values = harder walls
; Note for explicit time integration, dt < C_eta
C_eta=1.34e-3;
; [none, Insect, cylinder, two-cylinders, rotating_cylinder] if "Insect", all other parameters are read from "Insect" section.
geometry=cylinder;
; center coordinate of object
x_cntr=10.0 10.0 0;
; [triangle,rhombus,cylinder]: length or diameter of the object
length=1.0;


[Debug]
; check if the ghost node synchronization gives the right order, on a random
; grid. this test costs some CPU time but no memory. It is done only once at startup.
test_ghost_nodes_synch=1;
test_treecode=0;
